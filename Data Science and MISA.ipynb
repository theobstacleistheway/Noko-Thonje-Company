{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Science and MISA Zimbabwe\n",
    "\n",
    "##Natural Language Processing of Twitter Data from the Run-up to the 2018 Elections\n",
    "\n",
    "### Notebook by [Joseph Noko](josephnoko@gmail.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-80-a717203f8fbf>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-80-a717203f8fbf>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    **It is recommended to [view this notebook in nbviewer](http://nbviewer.ipython.org/github.com/theobstacleistheway/Noko-Thonje-Company/blob/master/Data%20Science%20and%20MISA%20Zimbabwe.ipynb) for the best viewing experience.**\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "**It is recommended to [view this notebook in nbviewer](http://nbviewer.ipython.org/github.com/theobstacleistheway/Noko-Thonje-Company/blob/master/Data%20Science%20and%20MISA%20Zimbabwe.ipynb) for the best viewing experience.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-81-a4d2fa2539a0>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-81-a4d2fa2539a0>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    <img src=\"MISA_logo_zimbabwe.png\" />\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<img src=\"MISA_logo_zimbabwe.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-82-0105175bb9dc>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-82-0105175bb9dc>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    3. [Get and Inspect the Data](#Get-and-Inspect-the-Data)\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "\n",
    "2. [License](#License)\n",
    "\n",
    "3. [Get and Inspect the Data](#Get-and-Inspect-the-Data)\n",
    "\n",
    "4. [Data Preprocessing](#Data-Preprocessing)\n",
    "\n",
    "5. [Normalizing Text](#Normalizing-Text)\n",
    "\n",
    "6. [Analysis](#Analysis)\n",
    "\n",
    "7. [Conclusions](#Conclusions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-83-a43ccbad908e>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-83-a43ccbad908e>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    [[ go back to the top ]](#Table-of-contents)\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Introduction\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "    \n",
    "The purpose of this Jupyter Notebook presentation is to introduce MISA Zimbabwe to data science. Data science is the study of the\n",
    "extraction of knowledge from data. It uses various techniques from many fields, including signal processing, mathematics,\n",
    "probability models, machine learning, computer programming, statistics, data engineering, pattern matching, data visualization, \n",
    "uncertainty modeling, data warehousing, and high performance computing with the goal of extracting useful knowledge from the data.\n",
    "Data Science is not restricted to only big data, although the fact that data is scaling up makes big data an important aspect of \n",
    "data science. \n",
    "\n",
    "Data scientists solve complicated data problems using mathematics, statistics and computer science, although very good skill in \n",
    "these subjects are not required.[1] However, a data scientist is most likely to be an expert in only one or two of these \n",
    "disciplines, meaning that cross disciplinary teams can be a key component of data science.\n",
    "\n",
    "Good data scientists are able to apply their skills to achieve a broad spectrum of end results. The skill-sets and competencies \n",
    "that data scientists employ vary widely.  \n",
    "\n",
    "We believe that by using data science, MISA Zimbabwe can learn more about how ZImbabweans are utilising what \n",
    "freedom of expression they have, and learn about the various issues that Zimbabweans who use social media, are discussing. In order\n",
    "to achieve this aim, we will go through a standard and basic workflow to demonstrate ways in which data science can be used. The \n",
    "subject of this workflow will be the recently passed general elections. \n",
    "\n",
    "Researchers have used Twitter for a huge range of subjects: [measuring how people cope with global crises and their aftermaths](https://www.unglobalpulse.org/projects/twitter-and-perceptions-crisis-related-stress), tracking\n",
    "[geographic differences in public health](http://www.cs.jhu.edu/~mpaul/files/2011.icwsm.twitter_health.pdf) and [analyzing the behavior of automated “bot” accounts](https://regmedia.co.uk/2016/10/19/data-memo-first-presidential-debate.pdf) during the 2016 presidential \n",
    "debates, to name a few. At least 25 billion tweets were collected and analyzed in scholarly research published from 2007 to 2012,\n",
    "according to [a paper Proferes published with a colleague](https://www.emeraldinsight.com/doi/abs/10.1108/AJIM-09-2013-0083?journalCode=ajim) in 2014. They counted 382 papers in just the first six years that \n",
    "Twitter existed. Katrin Weller, an information scientist working at the GESIS Leibniz Institute for the Social Sciences in \n",
    "Germany, [published a separate paper](https://www.ssoar.info/ssoar/bitstream/handle/document/47768/ssoar-knoworg-2014-3-weller-What_do_we_get_from.pdf?sequence=1) in 2014 that also tried to count social science research papers dealing with Twitter, and she\n",
    "came up with a similar number, as illustrated in the graph below. Proferes and Weller agreed that many more Twitter-based papers have been published since.\n",
    "\n",
    "<img src=\"mkb-twitterreasearch-1.png\" />\n",
    "    \n",
    "Twitter is a preferred source of social media data for research because the website is relatively easy and cheap for scientists \n",
    "to use. Twitter is basically set up as a self-publishing platform. Unless a user makes his or her account private — and the vast \n",
    "majority do not — everything posted is public information, just as if you printed it up on a flyer or yelled it loudly in the\n",
    "town square. But Twitter posts are a lot more useful for data analysis than a guy wandering around screaming, “’Tis 6 o’clock and\n",
    "all is #blessed!”\n",
    "    \n",
    "Using [Twitter’s own systems or third-party apps](https://gwu-libraries.github.io/sfm-ui/posts/2017-09-14-twitter-data) that scrape data from the site, scientists can get [free samples of tweets](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5639727/) — [drawn\n",
    "at random](http://socialmedia-class.org/twittertutorial.html) either from the site’s ongoing daily stream of other people’s consciousness or (using keywords and [other search \n",
    "parameters](https://developer.twitter.com/en/docs/tweets/rules-and-filtering/overview/premium-operators)) from the archive of public tweets that go back years. With bigger budgets, scientists can pay for larger collections \n",
    "of tweets. And, since Twitter is predominantly a text-based medium, it’s easier to analyze and compare those messages with one \n",
    "another than it would be to study Instagram, which has a similar level of public availability, Cesare said. Earlier this week, \n",
    "[Twitter announced that it would begin to put some limits](https://blog.twitter.com/developer/en_us/topics/tools/2018/new-developer-requirements-to-protect-our-platform.html) on how many of these requests people can make and which apps they can \n",
    "use to make them — but, in general, Proferes said it’s still one of the easiest social media sites to use for research.\n",
    "\n",
    "Some researchers have used Twitter to amass huge data sets. Proferes found nine papers published between 2007 and 2012 that were \n",
    "based on collections of more than 1 billion tweets each. Jennifer Van Hook, a professor of sociology and demography at Penn \n",
    "State, is part of a research team that has collected 30 terabytes of geotagged tweets — something the university has promoted as \n",
    "[“the largest publicly accessible archive of human behavior in existence.”](https://news.psu.edu/story/474782/2017/07/17/research/twitter-data-changing-future-population-research) One use for this data set: improving the quality of \n",
    "other Twitter-based social science research by figuring out how well the population of Twitter matches the population of a given \n",
    "geographic area. Eventually, Van Hook told me, the team hopes to create tools that allow researchers to statistically account for\n",
    "differences between physical and online communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-84-b05ded0aa481>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-84-b05ded0aa481>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    [[ go back to the top ]](#Table-of-contents)\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## License\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "Please see the [repository README file](https://github.com/theobstacleistheway/Noko-Thonje-Company/blob/master/README.md) for the\n",
    "licenses and usage terms for the instructional material and code in this notebook. In general, I have licensed this material so \n",
    "that it is as widely usable and shareable as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-85-c9fa9fadda45>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-85-c9fa9fadda45>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    [[ go back to the top ]](#Table-of-contents)\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Get and Inspect the Data\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "    \n",
    "For the purposes of this exercise, we will use the data mined by Mr. Kuda Hove. The data set is \n",
    "very small, so what we can draw from the data set is very limited, but, can open the window, \n",
    "however small the openining, into understanding what data science can help us with. The period \n",
    "covered is from the 18th to the 25th of July 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\"\"\" Load and concatenate multiple csv files with Twitter data \"\"\"\n",
    "    \n",
    "# Read tweet data (obtained from Mr. K. Hove)\n",
    "    \n",
    "df1 = pd.read_csv('Election-Week-Data (18_19-07-2018+G).csv', sep=',', encoding = 'latin 1', header=None, na_values=['NA'], names=[\"Timestamp\", \"Additional Info'\", \"Location\", \"User\", \"Screen Name\", \"Tweet\"])\n",
    "df2 = pd.read_csv('Election-Week-Data (21_23-07-2018+G).csv', sep=',', encoding = 'latin 1', header=None, na_values=['NA'], names=[\"Timestamp\", \"Additional Info'\", \"Location\", \"User\", \"Screen Name\", \"Tweet\"])\n",
    "df3 = pd.read_csv('Election-Week-Data (24_25-07-2018+G).csv', sep=',', encoding = 'latin 1', header=None, na_values=['NA'], names=[\"Timestamp\", \"Additional Info'\", \"Location\", \"User\", \"Screen Name\", \"Tweet\"])\n",
    "    \n",
    "# Concatenate data\n",
    "    \n",
    "frames = [df1, df2, df3]\n",
    "new_df1 = pd.concat(frames)\n",
    "    \n",
    "# Drop irrelevant columns\n",
    "    \n",
    "del new_df1[\"Additional Info'\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Location</th>\n",
       "      <th>User</th>\n",
       "      <th>Screen Name</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-07-18 23:59:28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Insola</td>\n",
       "      <td>Zim_Ozil</td>\n",
       "      <td>RT @Taw_1987: It's your time #ED. Zimbabwe nee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-07-18 23:57:16</td>\n",
       "      <td>Mashonaland East, Zimbabwe</td>\n",
       "      <td>Miss_Ndandi</td>\n",
       "      <td>MPrezha</td>\n",
       "      <td>@girlwithacrushh @ConcernedZimCit Aizve mainin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-07-18 23:55:57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ana</td>\n",
       "      <td>Ana55076611</td>\n",
       "      <td>Mom just told me to never get fat. She doesn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-07-18 23:44:49</td>\n",
       "      <td>Jhb,South Africa.</td>\n",
       "      <td>C-life4real</td>\n",
       "      <td>cmhembs</td>\n",
       "      <td>RT @DailyNewsZim: Today's #DailyNews front pag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-07-18 23:44:39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm Zimbabweð¿ð¼</td>\n",
       "      <td>ImZimbabwe</td>\n",
       "      <td>RT @povozim: Nelson Chamisa closing prayer at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-07-18 23:43:22</td>\n",
       "      <td>Jhb,South Africa.</td>\n",
       "      <td>C-life4real</td>\n",
       "      <td>cmhembs</td>\n",
       "      <td>RT @ItsMutai: Pres @edmnangagwa of Zimbabwe is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-07-18 23:37:38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zcash Foundation</td>\n",
       "      <td>ZcashFoundation</td>\n",
       "      <td>RT @OB1Company: Zcon0 Recap - A 3-Day Look at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-07-18 23:37:37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ana</td>\n",
       "      <td>Ana55076611</td>\n",
       "      <td>I've been eating a lot. Too much, I'll get bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-07-18 23:34:03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jones Musara</td>\n",
       "      <td>JonesMusara</td>\n",
       "      <td>@DrNkuSibanda you are a desperate loser. No la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-07-18 23:32:23</td>\n",
       "      <td>BULAWAYO</td>\n",
       "      <td>mlungisi mazhale</td>\n",
       "      <td>zhizzy91</td>\n",
       "      <td>RT @seshndlovini: One canât surely request t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Timestamp                    Location                  User  \\\n",
       "0  2018-07-18 23:59:28                         NaN                Insola   \n",
       "1  2018-07-18 23:57:16  Mashonaland East, Zimbabwe           Miss_Ndandi   \n",
       "2  2018-07-18 23:55:57                         NaN                   Ana   \n",
       "3  2018-07-18 23:44:49           Jhb,South Africa.           C-life4real   \n",
       "4  2018-07-18 23:44:39                         NaN  I'm Zimbabweð¿ð¼   \n",
       "5  2018-07-18 23:43:22           Jhb,South Africa.           C-life4real   \n",
       "6  2018-07-18 23:37:38                         NaN      Zcash Foundation   \n",
       "7  2018-07-18 23:37:37                         NaN                   Ana   \n",
       "8  2018-07-18 23:34:03                         NaN          Jones Musara   \n",
       "9  2018-07-18 23:32:23                    BULAWAYO      mlungisi mazhale   \n",
       "\n",
       "       Screen Name                                              Tweet  \n",
       "0         Zim_Ozil  RT @Taw_1987: It's your time #ED. Zimbabwe nee...  \n",
       "1          MPrezha  @girlwithacrushh @ConcernedZimCit Aizve mainin...  \n",
       "2      Ana55076611  Mom just told me to never get fat. She doesn't...  \n",
       "3          cmhembs  RT @DailyNewsZim: Today's #DailyNews front pag...  \n",
       "4       ImZimbabwe  RT @povozim: Nelson Chamisa closing prayer at ...  \n",
       "5          cmhembs  RT @ItsMutai: Pres @edmnangagwa of Zimbabwe is...  \n",
       "6  ZcashFoundation  RT @OB1Company: Zcon0 Recap - A 3-Day Look at ...  \n",
       "7      Ana55076611  I've been eating a lot. Too much, I'll get bac...  \n",
       "8      JonesMusara  @DrNkuSibanda you are a desperate loser. No la...  \n",
       "9         zhizzy91  RT @seshndlovini: One canât surely request t...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-88-ca0c5d3cba78>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-88-ca0c5d3cba78>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    The above represents the first 10 tweets of the entire, concacatanated data set.\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "The above represents the first 10 tweets of the entire, concacatanated data set. \n",
    "\n",
    "The data given is in a usable format. The first row in the data file defines the column headers,  and the headers are descriptive\n",
    "enough for us to understand what each column represents.\n",
    "\n",
    "Each row following the first row represents the elements of a single tweet: timestamp, location  of user, the name of the user, \n",
    "the user's screen name and the body of the tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "    \n",
    "Before we proceed any further, we will have to clean the raw text, which means splitting it into text and handling puctuation and\n",
    "case.\n",
    "\n",
    "We are particularly interested in the section with the corpus of the tweet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = new_df1[\"Tweet\"]\n",
    "string_raw = ''.join(str(x) for x in raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(string_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2283195\n"
     ]
    }
   ],
   "source": [
    "# Total Words in Data\n",
    "print(len(string_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @Taw_1987: It's your time #ED. Zimbabwe needs peace and prosperity. #ZimElections2018 #ZimPreside\n"
     ]
    }
   ],
   "source": [
    "# First 100 characters in the corpus\n",
    "print(string_raw[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenization to produce a list of words and punctuation\n",
    "#We want to break up the string into words and punctuation\n",
    "import nltk\n",
    "\n",
    "tokens = nltk.word_tokenize(string_raw)\n",
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418630"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT', '@', 'Taw_1987', ':', 'It', \"'s\", 'your', 'time', '#', 'ED']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.text.Text"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take the further step of creating an NLTK text from this list\n",
    "text = nltk.Text(tokens)\n",
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['she',\n",
       " 'was',\n",
       " 'ZEC',\n",
       " 'Chairperson',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/rjRVmaVZ4d',\n",
       " 'Post',\n",
       " 'made',\n",
       " 'by',\n",
       " 'creaâ\\x80¦RT',\n",
       " '@',\n",
       " 'ZESN1',\n",
       " ':',\n",
       " 'Press',\n",
       " 'statement',\n",
       " 'on',\n",
       " 'the',\n",
       " 'Ballot',\n",
       " 'Paper',\n",
       " '#',\n",
       " 'ZimVotes2018',\n",
       " '#',\n",
       " 'ZimDecides2018',\n",
       " '@',\n",
       " 'RindaiVava',\n",
       " '@',\n",
       " 'ellendingani',\n",
       " '@',\n",
       " 'OpenParlyZw',\n",
       " '@',\n",
       " 'kubatana',\n",
       " '@',\n",
       " '263Chat',\n",
       " '@',\n",
       " 'Eleâ\\x80¦RT',\n",
       " '@',\n",
       " 'ChiwaraSarah',\n",
       " ':',\n",
       " '``']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can slice the text to see inside it and perform other actions\n",
    "text[1020:1060]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumiso Dabengwa; MDC Alliance; July 2018; Nelson Chamisa; solid\n",
      "backing; Sibangilizwe Nkomo..son; Joshua Nkomo.The; Godisinit https;\n",
      "Gweru.Mkoba Stadium; Stadium full; Itâs done; Nkomo.The\n",
      "momentuâ¦RT; oversubscribed SMART; night.The mood; SMART policy;\n",
      "Alliance rally; White City; Business Community; policy meeting; last\n",
      "night.The\n"
     ]
    }
   ],
   "source": [
    "text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-98-899985648d1f>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-98-899985648d1f>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    [[ go back to the top ]](#Table-of-contents)\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Normalizing Text\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "#Convert text to lowercase\n",
    "#We should ignore the case distinction by normalizing everything to lowercase, and filter out non-alphabetic characters\n",
    "fdist = nltk.FreqDist(ch.lower() for ch in text if ch.isalpha())\n",
    "fdist.most_common(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analysis\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "    \n",
    "#We can use the above to create a table of the top 20 mentions of politicial actors \n",
    "mentions = pd.read_csv('mentions.csv')\n",
    "mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A simple analysis of the above shows that Chamisa/MDC mentions numbered 8281 in frequency whereas Mnangagwa/ZANU PF mentions \n",
    "had a frequency of 6879 over the interval in which the data was taken. This is not an indication of where election results \n",
    "would be given the small sample size and the fact  that ZANU PF's electorate is mostly rural, but it does shoiw the dominance \n",
    "of the opposition in social media. In the sample, rt at the end of what are hashtags, e.g., edhasmyvotert, indicates that the \n",
    "hashtag was retweeted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conclusions\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "    \n",
    "Our technology identifies the topics driving public sentiment, helping you to understand not just how people feel, but what’s \n",
    "making them feel that way. Our team of human verifiers categorise individual social media posts into a number of \n",
    "industry-specific categories from financial services to telcos. Our topics-level analysis empowers you to make tailored \n",
    "interventions from customer experience to political campaigning.\n",
    "    \n",
    "Based on predefined criteria, public posts from across social media are gathered. Using sophisticated machine learning \n",
    "algorithms, each post is evaluated to ensure relevance and the discovery of useful metadata such as location.\n",
    "\n",
    "A sample of the social media data is distributed to our team of trained human contributors. The team verify the relevancy of the\n",
    "data and then determine the sentiment contained in the post as being positive, negative, or neutral.\n",
    "\n",
    "Our team categorises the individual mentions into one or more topic categories. The categorises are tailored to particular\n",
    "industry needs from financial services to retail outlets.\n",
    "    \n",
    "Algorithms by themselves cannot accurately understand human conversation with its tonal subtleties, sarcasm, slang, and mixed \n",
    "sentiment. Noko, THonje & Company overcomes these challenges by augmenting its algorithms with a proprietary team of trained \n",
    "human contributors. Team members review, verify and categorise the sentiment contained in individual social media posts, helping\n",
    "us to achieve industry-leading sentiment accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
